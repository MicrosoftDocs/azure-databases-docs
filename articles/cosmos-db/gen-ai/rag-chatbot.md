---
title: 'Tutorial: Build a RAG Chatbot'
titleSuffix: Azure Cosmos DB for NoSQL
description: Build a retrieval augmented generation (RAG) chatbot in Python using Azure Cosmos DB for NoSQL's vector search capabilities.
author: TheovanKraay
ms.author: thvankra
ms.service: azure-cosmos-db
ms.subservice: nosql
ms.topic: tutorial
ms.date: 10/20/2025
ms.update-cycle: 180-days
ms.collection:
  - ce-skilling-ai-copilot
appliesto:
  - âœ… NoSQL
ms.custom:
  - sfi-ropc-nochange
---

# Tutorial: Build a RAG chatbot with Azure Cosmos DB NoSQL API

In this guide, we demonstrate how to build a [retrieval-augmented generation (RAG)](../gen-ai/rag.md) application using a subset of the Movie Lens dataset. This sample uses the Azure Cosmos DB Python SDK for NoSQL to perform vector search for RAG, store and retrieve chat history, and store vectors of the chat history to use as a semantic cache. Azure OpenAI is used to generate embeddings and large language model (LLM) completions.

At the end, we create a simple user interface to allow users to type in questions and display responses generated by Azure OpenAI or served from the cache. The responses also display an elapsed time to show the effect caching has on performance versus generating a response.

## Prerequisites

- Azure Cosmos DB for NoSQL account

  - [Enabled `EnableNoSQLVectorSearch` capability](../nosql/vector-search.md#enable-the-vector-indexing-and-search-feature)

- Azure OpenAI account

  - Deployment for embeddings using a model like `text-embedding-3-large`

  - Deployment for completions using a model like `gpt-35-turbo`

## Get prerequisite files

1. Start in an empty directory.

1. Navigate to the sample compressed folder on GitHub: (<https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples/blob/main/DataSet/Movies/MovieLens-4489-256D.zip>)

1. In the menu, select **Download**.

1. Save the file to your local project folder using the file name *MovieLens-4489-256D.zip*.

1. Navigate to the sample prevectorized JSON data on GitHub: (<https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples/blob/main/DataSet/Movies/MovieLens-4489-256D.json>)

1. In the menu, select **Download**.

1. Save the file to your local project folder using the file name *MovieLens-4489-256D.json*.

1. In the project directory, create a new folder or path named *Data/*.

    > [!IMPORTANT]
    > The name is case-sensitive.

## Configure project and directory

Start by configuring your Python project with the required packages and environment variables.

1. Open a terminal in the project directory.

1. Install the required Python packages using these shell commands.

    ```bash
    pip install --user python-dotenv
    pip install --user aiohttp
    pip install --user openai
    pip install --user gradio
    pip install --user ijson
    pip install --user nest_asyncio
    pip install --user tenacity
    pip install --user azure-cosmos
    ```
    
    > [!NOTE]
    > Ensure that you have `azure-cosmos` version 4.7 or higher installed.

1. Create an *.env* file.

1. Enter the following information into the file. Use the credentials and resource names for your existing Azure OpenAI and Azure Cosmos DB resources.

    ```env
    cosmos_uri = "https://<azure-cosmos-db-nosql-account-name>.documents.azure.com:443/"
    cosmos_key = "<azure-cosmos-db-nosql-account-key>"
    cosmos_database_name = "ragdatabase"
    cosmos_container_name = "vectorstorecontainer"
    cosmos_vector_property_name = "vector"
    cosmos_cache_database_name = "ragdatabase"
    cosmos_cache_container_name = "vectorcachecontainer"
    openai_endpoint = "<azure-openai-account-endpoint>"
    openai_key = "<azure-openai-account-key>"
    openai_type = "azure"
    openai_api_version = "2023-05-15"
    openai_embeddings_deployment = "<azure-openai-embeddings-deployment-name>"
    openai_embeddings_model = "<azure-openai-embeddings-model>"
    openai_embeddings_dimensions = "1536"
    openai_completions_deployment = "<azure-openai-completions-deployment-name>"
    openai_completions_model = "<azure-openai-completions-model>"
    ```

1. Create an *app.py* file.

## Initialize your client connection

Use this initial code to create a client application and initialize a connection to your resources in Azure.

```python
# Import the required libraries
import time
import json
import uuid
import urllib 
import ijson
import zipfile
from dotenv import dotenv_values
from openai import AzureOpenAI
from azure.core.exceptions import AzureError
from azure.cosmos import ThroughputProperties, PartitionKey, exceptions
from time import sleep
import gradio as gr

# Cosmos DB imports
from azure.cosmos import CosmosClient

# Load configuration from .env file
config = dotenv_values()

cosmos_conn = config['cosmos_uri']
cosmos_key = config['cosmos_key']
cosmos_database = config['cosmos_database_name']
cosmos_collection = config['cosmos_container_name']
cosmos_vector_property = config['cosmos_vector_property_name']
comsos_cache_db = config['cosmos_cache_database_name']
cosmos_cache = config['cosmos_cache_container_name']

# Create the Azure Cosmos DB for NoSQL async client for faster data loading
cosmos_client = CosmosClient(url=cosmos_conn, credential=cosmos_key)

openai_endpoint = config['openai_endpoint']
openai_key = config['openai_key']
openai_api_version = config['openai_api_version']
openai_embeddings_deployment = config['openai_embeddings_deployment']
openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])
openai_completions_deployment = config['openai_completions_deployment']

# Create the OpenAI client
openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_api_version)
```

## Create a database and containers with vector policies

This function takes a database object, a collection name, the name of the document property that stores vectors, and the number of vector dimensions used for the embeddings.

```python
db = cosmos_client.create_database_if_not_exists(cosmos_database)

# Create the vector embedding policy to specify vector details
vector_embedding_policy = {
    "vectorEmbeddings": [ 
        { 
            "path":"/" + cosmos_vector_property,
            "dataType":"float32",
            "distanceFunction":"cosine",
            "dimensions":openai_embeddings_dimensions
        }, 
    ]
}

# Create the vector index policy to specify vector details
indexing_policy = {
    "includedPaths": [ 
    { 
        "path": "/*" 
    } 
    ], 
    "excludedPaths": [ 
    { 
        "path": "/\"_etag\"/?",
        "path": "/" + cosmos_vector_property + "/*",
    } 
    ], 
    "vectorIndexes": [ 
        {
            "path": "/"+cosmos_vector_property, 
            "type": "quantizedFlat" 
        }
    ]
} 

# Create the data collection with vector index (note: this creates a container with an autoscale limit of 20,000 RUs to allow fast data load)
try:
    movies_container = db.create_container_if_not_exists(id=cosmos_collection, 
                                                  partition_key=PartitionKey(path='/id'),
                                                  indexing_policy=indexing_policy, 
                                                  vector_embedding_policy=vector_embedding_policy,
                                                  offer_throughput=ThroughputProperties(auto_scale_max_throughput=20000)) 
    print('Container with id \'{0}\' created'.format(movies_container.id)) 

except exceptions.CosmosHttpResponseError: 
    raise 

# Create the cache collection with vector index
try:
    cache_container = db.create_container_if_not_exists(id=cosmos_cache, 
                                                  partition_key=PartitionKey(path='/id'), 
                                                  indexing_policy=indexing_policy,
                                                  vector_embedding_policy=vector_embedding_policy,
                                                  offer_throughput=ThroughputProperties(auto_scale_max_throughput=2000)) 
    print('Container with id \'{0}\' created'.format(cache_container.id)) 

except exceptions.CosmosHttpResponseError: 
    raise
```

## Generate embeddings from Azure OpenAI

This function upserts the user input for vector search. Ensure the dimensionality and model used match the sample data provided, or else regenerate vectors with your desired model.

```python
from tenacity import retry, stop_after_attempt, wait_random_exponential 
import logging

@retry(wait=wait_random_exponential(min=2, max=300), stop=stop_after_attempt(20))
def generate_embeddings(text):
    try:        
        response = openai_client.embeddings.create(
            input=text,
            model=openai_embeddings_deployment,
            dimensions=openai_embeddings_dimensions
        )
        embeddings = response.model_dump()
        return embeddings['data'][0]['embedding']
    except Exception as e:
        # Log the exception with traceback for easier debugging
        logging.error("An error occurred while generating embeddings.", exc_info=True)
        raise
```

## Load data from the JSON file

Extract the prevectorized MovieLens dataset from the compressed folder and JSON data file.

```python
# Unzip the data file
with zipfile.ZipFile("MovieLens-4489-256D.zip", 'r') as zip_ref:
    zip_ref.extractall("Data")
zip_ref.close()

# Load the data file
data = []
with open('MovieLens-4489-256D.json', 'r') as d:
    data = json.load(d)

# View the number of documents in the data (4489)
len(data)
```

## Store data in Azure Cosmos DB

Upsert data into Azure Cosmos DB for NoSQL. Records are written asynchronously.

```python
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

async def insert_data():
    start_time = time.time()  # Record the start time
    
    counter = 0
    tasks = []
    max_concurrency = 4  # Adjust this value to control the level of concurrency
    semaphore = asyncio.Semaphore(max_concurrency)
    print("Starting doc load, please wait...")
    
    def upsert_item_sync(obj):
        movies_container.upsert_item(body=obj)
    
    async def upsert_object(obj):
        nonlocal counter
        async with semaphore:
            await asyncio.get_event_loop().run_in_executor(None, upsert_item_sync, obj)
            # Progress reporting
            counter += 1
            if counter % 100 == 0:
                print(f"Sent {counter} documents for insertion into collection.")
    
    for obj in data:
        tasks.append(asyncio.create_task(upsert_object(obj)))
    
    # Run all upsert tasks concurrently within the limits set by the semaphore
    await asyncio.gather(*tasks)
    
    end_time = time.time()  # Record the end time
    duration = end_time - start_time  # Calculate the duration
    print(f"All {counter} documents inserted!")
    print(f"Time taken: {duration:.2f} seconds ({duration:.3f} milliseconds)")


# Run the async function
asyncio.run(insert_data())
```

## Perform vector search

This function defines a vector search over the movies data and chat cache collections.

```python
def vector_search(container, vectors, similarity_score=0.02, num_results=5):
    results = container.query_items(
        query='''
        SELECT TOP @num_results c.overview, VectorDistance(c.vector, @embedding) as SimilarityScore 
        FROM c
        WHERE VectorDistance(c.vector,@embedding) > @similarity_score
        ORDER BY VectorDistance(c.vector,@embedding)
        ''',
        parameters=[
            {"name": "@embedding", "value": vectors},
            {"name": "@num_results", "value": num_results},
            {"name": "@similarity_score", "value": similarity_score}
        ],
        enable_cross_partition_query=True,
        populate_query_metrics=True
    )
    results = list(results)
    formatted_results = [{'SimilarityScore': result.pop('SimilarityScore'), 'document': result} for result in results]

    return formatted_results
```

## Get recent chat history

This function provides conversational context to the LLM, allowing it to better have a conversation with the user.

```python
def get_chat_history(container, completions=3):
    results = container.query_items(
        query='''
        SELECT TOP @completions *
        FROM c
        ORDER BY c._ts DESC
        ''',
        parameters=[
            {"name": "@completions", "value": completions},
        ],
        enable_cross_partition_query=True
    )
    results = list(results)
    return results
```

## Chat completion functions

Define the functions to handle the chat completion process, including caching responses.

```python
def generate_completion(user_prompt, vector_search_results, chat_history):
    system_prompt = '''
    You are an intelligent assistant for movies. You are designed to provide helpful answers to user questions about movies in your database.
    You are friendly, helpful, and informative and can be lighthearted. Be concise in your responses, but still friendly.
     - Only answer questions related to the information provided below. Provide at least 3 candidate movie answers in a list.
     - Write two lines of whitespace between each answer in the list.
    '''

    messages = [{'role': 'system', 'content': system_prompt}]
    for chat in chat_history:
        messages.append({'role': 'user', 'content': chat['prompt'] + " " + chat['completion']})
    messages.append({'role': 'user', 'content': user_prompt})
    for result in vector_search_results:
        messages.append({'role': 'system', 'content': json.dumps(result['document'])})

    response = openai_client.chat.completions.create(
        model=openai_completions_deployment,
        messages=messages,
        temperature=0.1
    )    
    return response.model_dump()

def chat_completion(cache_container, movies_container, user_input):
    print("starting completion")
    # Generate embeddings from the user input
    user_embeddings = generate_embeddings(user_input)
    # Query the chat history cache first to see if this question has been asked before
    cache_results = get_cache(container=cache_container, vectors=user_embeddings, similarity_score=0.99, num_results=1)
    if len(cache_results) > 0:
        print("Cached Result\n")
        return cache_results[0]['completion'], True
        
    else:
        # Perform vector search on the movie collection
        print("New result\n")
        search_results = vector_search(movies_container, user_embeddings)

        print("Getting Chat History\n")
        # Chat history
        chat_history = get_chat_history(cache_container, 3)
        # Generate the completion
        print("Generating completions \n")
        completions_results = generate_completion(user_input, search_results, chat_history)

        print("Caching response \n")
        # Cache the response
        cache_response(cache_container, user_input, user_embeddings, completions_results)

        print("\n")
        # Return the generated LLM completion
        return completions_results['choices'][0]['message']['content'], False
```

## Cache generated responses

Save the user prompts and generated completions to the cache for faster future responses.

```python
def cache_response(container, user_prompt, prompt_vectors, response):
    chat_document = {
        'id': str(uuid.uuid4()),
        'prompt': user_prompt,
        'completion': response['choices'][0]['message']['content'],
        'completionTokens': str(response['usage']['completion_tokens']),
        'promptTokens': str(response['usage']['prompt_tokens']),
        'totalTokens': str(response['usage']['total_tokens']),
        'model': response['model'],
        'vector': prompt_vectors
    }
    container.create_item(body=chat_document)

def get_cache(container, vectors, similarity_score=0.0, num_results=5):
    # Execute the query
    results = container.query_items(
        query= '''
        SELECT TOP @num_results *
        FROM c
        WHERE VectorDistance(c.vector,@embedding) > @similarity_score
        ORDER BY VectorDistance(c.vector,@embedding)
        ''',
        parameters=[
            {"name": "@embedding", "value": vectors},
            {"name": "@num_results", "value": num_results},
            {"name": "@similarity_score", "value": similarity_score},
        ],
        enable_cross_partition_query=True, populate_query_metrics=True)
    results = list(results)
    return results
```

## Create a user interface

Build a user interface for interacting with the AI application.

```python
chat_history = []

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(label="Cosmic Movie Assistant", type="tuples")
    msg = gr.Textbox(label="Ask me about movies in the Cosmic Movie Database!")
    clear = gr.Button("Clear")

    def user(user_message, chat_history):
        start_time = time.time()
        response_payload, cached = chat_completion(cache_container, movies_container, user_message)
        end_time = time.time()
        elapsed_time = round((end_time - start_time) * 1000, 2)
        details = f"\n (Time: {elapsed_time}ms)"
        if cached:
            details += " (Cached)"
        chat_history.append([user_message, response_payload + details])
        
        return gr.update(value=""), chat_history
    
    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)
    clear.click(lambda: None, None, chatbot, queue=False)

# Launch the Gradio interface
demo.launch(debug=True, share=False)

# Be sure to run this cell to close or restart the Gradio demo
demo.close()
```

## Run the application

Run the application to start the web interface and interact with the dataset.

1. Run the project by executing the following command in your terminal from the project directory.

    ```bash
    python app.py
    ```

1. Observe the console output.

    ```output
    Container with id 'vectorstorecontainer' created
    Container with id 'vectorcachecontainer' created
    Starting doc load, please wait...
    Sent 100 documents for insertion into collection.
    Sent 200 documents for insertion into collection.
    Sent 300 documents for insertion into collection.
    Sent 400 documents for insertion into collection.
    Sent 500 documents for insertion into collection.
    ...
    Sent 4300 documents for insertion into collection.
    Sent 4400 documents for insertion into collection.
    All 4489 documents inserted!
    Time taken: 24.66 seconds (24.659 milliseconds)
    ...
    * Running on local URL:  http://127.0.0.1:7860
    ...
    ```

1. Navigate to the web interface using your browser.

1. Run any test prompt using the web interface.

    :::image type="content" source="media/rag-chatbot/web-prompt-interface.png" lightbox="media/rag-chatbot/web-prompt-interface.png" alt-text="Screenshot of the Cosmic Movie Assistant web interface with chat conversation and text input field.":::

1. Observe the extra console output.

    ```output
    ...
    New result
    
    Getting Chat History
    
    Generating completions 
    
    Caching response
    ...
    ```

1. Close the web interface and application.

## Related content

- [Python notebook](https://aka.ms/CosmosPythonRAGQuickstart).
- [Samples for Retrieval-Augmented LLMs](https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples)

